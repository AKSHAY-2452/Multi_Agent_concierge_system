{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIZJXb1Vb+aTgfvJqNHxp8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj3h5wIE-vD-",
        "outputId": "9177a304-626b-4e13-846c-5e08ee37a8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.42-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.42 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.42-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.84.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (3.11.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.11.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama-index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (4.14.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (1.17.2)\n",
            "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.32-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index) (1.20.0)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.42->llama-index) (0.16.0)\n",
            "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.32-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama-index) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama-index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama-index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama-index) (3.2.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.42->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.42-py3-none-any.whl (7.1 kB)\n",
            "Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.42-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.26-py3-none-any.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
            "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_parse-0.6.32-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.32-py3-none-any.whl (39 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, deprecated, colorama, aiosqlite, typing-inspect, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed aiosqlite-0.21.0 banks-2.1.2 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.26 llama-cloud-services-0.6.32 llama-index-0.12.42 llama-index-agent-openai-0.4.11 llama-index-cli-0.4.3 llama-index-core-0.12.42 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.7 llama-index-llms-openai-0.4.5 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.32 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-5.6.0 python-dotenv-1.1.0 striprtf-0.0.26 typing-inspect-0.9.0\n",
            "Collecting groq\n",
            "  Downloading groq-0.28.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.28.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.28.0\n",
            "Collecting llama-index-llms-groq\n",
            "  Downloading llama_index_llms_groq-0.3.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-groq) (0.12.42)\n",
            "Collecting llama-index-llms-openai-like<0.5,>=0.4.0 (from llama-index-llms-groq)\n",
            "  Downloading llama_index_llms_openai_like-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.11.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.17.2)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.4.5)\n",
            "Requirement already satisfied: transformers<5,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (4.52.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (4.3.8)\n",
            "Requirement already satisfied: openai<2,>=1.81.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (1.84.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.32.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.5.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (1.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-groq) (3.0.2)\n",
            "Downloading llama_index_llms_groq-0.3.2-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_llms_openai_like-0.4.0-py3-none-any.whl (4.6 kB)\n",
            "Installing collected packages: llama-index-llms-openai-like, llama-index-llms-groq\n",
            "Successfully installed llama-index-llms-groq-0.3.2 llama-index-llms-openai-like-0.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index\n",
        "%pip install groq\n",
        "!pip install llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "kwj5_1Zh_M3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field,ConfigDict\n",
        "\n",
        "from llama_index.core.tools import BaseTool\n",
        "\n",
        "class AgentConfig(BaseModel):\n",
        "\n",
        "\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "    name: str\n",
        "    description: str\n",
        "    system_prompt: str | None = None\n",
        "    tools: list[BaseTool] | None = None\n",
        "    tools_requiring_human_confirmation: list[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "V1etIsbw_RXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add_two_numbers(a: int, b: int) -> int:\n",
        "    return a + b\n",
        "\n",
        "def multiply_two_numbers(a: int, b: int) -> int:\n",
        "    return a * b\n",
        "\n",
        "\n",
        "add_two_numbers_tool = FunctionTool.from_defaults(fn=add_two_numbers)\n",
        "multiply_two_numbers_tool = FunctionTool.from_defaults(fn=multiply_two_numbers)\n",
        "\n",
        "agent_config = AgentConfig(\n",
        "    name=\"Addition Agent\",\n",
        "    description=\"Used to add two numbers together.\",\n",
        "    system_prompt=(\n",
        "    \"You are an AI agent ONLY capable of performing addition. \"\n",
        "    \"If the user asks anything outside of adding two numbers, respond with: \"\n",
        "    \"'Sorry, I can only help with addition problems.' \"\n",
        "    \"Do not answer unrelated questions.\"\n",
        "),\n",
        "    tools=[add_two_numbers_tool],\n",
        "    tools_requiring_human_confirmation=[\"add_two_numbers\"],\n",
        ")\n",
        "\n",
        "agent_config_2 = AgentConfig(\n",
        "    name=\"Multiplication Agent\",\n",
        "    description=\"Used to multiply two numbers together.\",\n",
        "    system_prompt=(\n",
        "    \"You are an AI agent ONLY capable of performing multiplication. \"\n",
        "    \"If the user asks anything outside of multiplying two numbers, respond with: \"\n",
        "    \"'Sorry, I can only help with multiplication problems.' \"\n",
        "    \"Do not answer unrelated questions.\"\n",
        "),\n",
        "    tools=[multiply_two_numbers_tool],\n",
        ")"
      ],
      "metadata": {
        "id": "cjj--i0Q_Y2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indicates completion of task and also handsover the control to next agent\n",
        "def request_transfer() -> None:\n",
        "    pass\n",
        "\n",
        "#Routes to a specific agent\n",
        "def transfer_to_agent(agent_name: str) -> None:\n",
        "    pass\n",
        "\n",
        "request_transfer_tool = FunctionTool.from_defaults(fn=request_transfer)\n",
        "transfer_to_agent_tool = FunctionTool.from_defaults(fn=transfer_to_agent)"
      ],
      "metadata": {
        "id": "UXYDvrBO_hJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "from llama_index.core.llms import ChatMessage, LLM\n",
        "from llama_index.core.program.function_program import get_function_tool\n",
        "from llama_index.core.tools import (\n",
        "    BaseTool,\n",
        "    ToolSelection,\n",
        ")\n",
        "from llama_index.core.workflow import (\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Context,\n",
        ")\n",
        "from llama_index.core.workflow.events import InputRequiredEvent, HumanResponseEvent\n",
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "\n",
        "class ActiveSpeakerEvent(Event):\n",
        "    pass\n",
        "\n",
        "\n",
        "class OrchestratorEvent(Event):\n",
        "    pass\n",
        "\n",
        "\n",
        "class ToolCallEvent(Event):\n",
        "    tool_call: ToolSelection\n",
        "    tools: list[BaseTool]\n",
        "\n",
        "\n",
        "class ToolCallResultEvent(Event):\n",
        "    chat_message: ChatMessage\n",
        "\n",
        "\n",
        "class ToolRequestEvent(InputRequiredEvent):\n",
        "    tool_name: str\n",
        "    tool_id: str\n",
        "    tool_kwargs: dict\n",
        "\n",
        "\n",
        "class ToolApprovedEvent(HumanResponseEvent):\n",
        "    tool_name: str\n",
        "    tool_id: str\n",
        "    tool_kwargs: dict\n",
        "    approved: bool\n",
        "    response: str | None = None\n",
        "\n",
        "\n",
        "class ProgressEvent(Event):\n",
        "    msg: str\n",
        "\n",
        "DEFAULT_ORCHESTRATOR_PROMPT = \"\"\"\n",
        "You are the orchestration agent.\n",
        "\n",
        "Your task is to choose the most appropriate agent from the list below to respond to the user's query.\n",
        "\n",
        "- Only select an agent if the query clearly relates to the agent's task.\n",
        "- If no agent is suitable, respond exactly with: \"I can't answer this\".\n",
        "\n",
        "Agents:\n",
        "{agent_context_str}\n",
        "\n",
        "User state:\n",
        "{user_state_str}\n",
        "\n",
        "Now, return ONLY the selected agent name, or \"I can't answer this\".\n",
        "\"\"\"\n",
        "\n",
        "DEFAULT_TOOL_REJECT_STR = \"The tool call was not approved, likely due to a mistake or preconditions not being met.\"\n",
        "\n",
        "class ConciergeAgent(Workflow):\n",
        "    def __init__(\n",
        "        self,\n",
        "        orchestrator_prompt: str | None = None,\n",
        "        default_tool_reject_str: str | None = None,\n",
        "        **kwargs: Any,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.orchestrator_prompt = orchestrator_prompt or DEFAULT_ORCHESTRATOR_PROMPT\n",
        "        self.default_tool_reject_str = (\n",
        "            default_tool_reject_str or DEFAULT_TOOL_REJECT_STR\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def setup(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> ActiveSpeakerEvent | OrchestratorEvent:\n",
        "        active_speaker = await ctx.get(\"active_speaker\", default=\"\")\n",
        "        user_msg = ev.get(\"user_msg\")\n",
        "        agent_configs = ev.get(\"agent_configs\", default=[])\n",
        "        llm: LLM = ev.get(\"llm\", default=Groq(model=\"llama3-70b-8192\", temperature=0.3))\n",
        "        chat_history = ev.get(\"chat_history\", default=[])\n",
        "        initial_state = ev.get(\"initial_state\", default={})\n",
        "        if (\n",
        "            user_msg is None\n",
        "            or agent_configs is None\n",
        "            or llm is None\n",
        "            or chat_history is None\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"User message, agent configs, llm, and chat_history are required!\"\n",
        "            )\n",
        "\n",
        "        if not llm.metadata.is_function_calling_model:\n",
        "            raise ValueError(\"LLM must be a function calling model!\")\n",
        "\n",
        "\n",
        "        agent_configs_dict = {ac.name: ac for ac in agent_configs}\n",
        "        await ctx.set(\"agent_configs\", agent_configs_dict)\n",
        "        await ctx.set(\"llm\", llm)\n",
        "\n",
        "        chat_history.append(ChatMessage(role=\"user\", content=user_msg))\n",
        "        await ctx.set(\"chat_history\", chat_history)\n",
        "\n",
        "        await ctx.set(\"user_state\", initial_state)\n",
        "\n",
        "\n",
        "        if active_speaker:\n",
        "            return ActiveSpeakerEvent()\n",
        "\n",
        "\n",
        "        return OrchestratorEvent(user_msg=user_msg)\n",
        "\n",
        "    @step\n",
        "    async def speak_with_agent(\n",
        "        self, ctx: Context, ev: ActiveSpeakerEvent\n",
        "    ) -> ToolCallEvent | ToolRequestEvent | StopEvent:\n",
        "\n",
        "        active_speaker = await ctx.get(\"active_speaker\")\n",
        "\n",
        "        agent_config: AgentConfig = (await ctx.get(\"agent_configs\"))[active_speaker]\n",
        "        chat_history = await ctx.get(\"chat_history\")\n",
        "        llm = await ctx.get(\"llm\")\n",
        "\n",
        "        user_state = await ctx.get(\"user_state\")\n",
        "        user_state_str = \"\\n\".join([f\"{k}: {v}\" for k, v in user_state.items()])\n",
        "        system_prompt = (\n",
        "            agent_config.system_prompt.strip()\n",
        "            + f\"\\n\\nHere is the current user state:\\n{user_state_str}\"\n",
        "        )\n",
        "\n",
        "        llm_input = [ChatMessage(role=\"system\", content=system_prompt)] + chat_history\n",
        "\n",
        "\n",
        "        tools = [request_transfer_tool] + agent_config.tools\n",
        "\n",
        "        response = await llm.achat_with_tools(tools, chat_history=llm_input)\n",
        "\n",
        "        tool_calls: list[ToolSelection] = llm.get_tool_calls_from_response(\n",
        "            response, error_on_no_tool_call=False\n",
        "        )\n",
        "        if len(tool_calls) == 0:\n",
        "            chat_history.append(response.message)\n",
        "            await ctx.set(\"chat_history\", chat_history)\n",
        "            return StopEvent(\n",
        "                result={\n",
        "                    \"response\": response.message.content,\n",
        "                    \"chat_history\": chat_history,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            if tool_call.tool_name == request_transfer_tool.metadata.name:\n",
        "                await ctx.set(\"active_speaker\", None)\n",
        "                ctx.write_event_to_stream(\n",
        "                    ProgressEvent(msg=\"Agent is requesting a transfer. Please hold.\")\n",
        "                )\n",
        "                return OrchestratorEvent()\n",
        "            elif tool_call.tool_name in agent_config.tools_requiring_human_confirmation:\n",
        "                ctx.write_event_to_stream(\n",
        "                    ToolRequestEvent(\n",
        "                        prefix=f\"Tool {tool_call.tool_name} requires human approval.\",\n",
        "                        tool_name=tool_call.tool_name,\n",
        "                        tool_kwargs=tool_call.tool_kwargs,\n",
        "                        tool_id=tool_call.tool_id,\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                ctx.send_event(\n",
        "                    ToolCallEvent(tool_call=tool_call, tools=agent_config.tools)\n",
        "                )\n",
        "\n",
        "        chat_history.append(response.message)\n",
        "        await ctx.set(\"chat_history\", chat_history)\n",
        "\n",
        "    @step\n",
        "    async def handle_tool_approval(\n",
        "        self, ctx: Context, ev: ToolApprovedEvent\n",
        "    ) -> ToolCallEvent | ToolCallResultEvent:\n",
        "\n",
        "        if ev.approved:\n",
        "            active_speaker = await ctx.get(\"active_speaker\")\n",
        "            agent_config = (await ctx.get(\"agent_configs\"))[active_speaker]\n",
        "            return ToolCallEvent(\n",
        "                tools=agent_config.tools,\n",
        "                tool_call=ToolSelection(\n",
        "                    tool_id=ev.tool_id,\n",
        "                    tool_name=ev.tool_name,\n",
        "                    tool_kwargs=ev.tool_kwargs,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            return ToolCallResultEvent(\n",
        "                chat_message=ChatMessage(\n",
        "                    role=\"tool\",\n",
        "                    content=ev.response or self.default_tool_reject_str,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    @step(num_workers=4)\n",
        "    async def handle_tool_call(\n",
        "        self, ctx: Context, ev: ToolCallEvent\n",
        "    ) -> ActiveSpeakerEvent:\n",
        "\n",
        "        tool_call = ev.tool_call\n",
        "        tools_by_name = {tool.metadata.get_name(): tool for tool in ev.tools}\n",
        "\n",
        "        tool_msg = None\n",
        "\n",
        "        tool = tools_by_name.get(tool_call.tool_name)\n",
        "        additional_kwargs = {\n",
        "            \"tool_call_id\": tool_call.tool_id,\n",
        "            \"name\": tool.metadata.get_name(),\n",
        "        }\n",
        "        if not tool:\n",
        "            tool_msg = ChatMessage(\n",
        "                role=\"tool\",\n",
        "                content=f\"Tool {tool_call.tool_name} does not exist\",\n",
        "                additional_kwargs=additional_kwargs,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            tool_output = await tool.acall(**tool_call.tool_kwargs)\n",
        "\n",
        "            tool_msg = ChatMessage(\n",
        "                role=\"tool\",\n",
        "                content=tool_output.content,\n",
        "                additional_kwargs=additional_kwargs,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            tool_msg = ChatMessage(\n",
        "                role=\"tool\",\n",
        "                content=f\"Encountered error in tool call: {e}\",\n",
        "                additional_kwargs=additional_kwargs,\n",
        "            )\n",
        "\n",
        "        ctx.write_event_to_stream(\n",
        "            ProgressEvent(\n",
        "                msg=f\"Tool {tool_call.tool_name} called with {tool_call.tool_kwargs} returned {tool_msg.content}\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return ToolCallResultEvent(chat_message=tool_msg)\n",
        "\n",
        "    @step\n",
        "    async def aggregate_tool_results(\n",
        "        self, ctx: Context, ev: ToolCallResultEvent\n",
        "    ) -> ActiveSpeakerEvent:\n",
        "\n",
        "        num_tool_calls = await ctx.get(\"num_tool_calls\")\n",
        "        results = ctx.collect_events(ev, [ToolCallResultEvent] * num_tool_calls)\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        chat_history = await ctx.get(\"chat_history\")\n",
        "        for result in results:\n",
        "            chat_history.append(result.chat_message)\n",
        "        await ctx.set(\"chat_history\", chat_history)\n",
        "\n",
        "        return ActiveSpeakerEvent()\n",
        "\n",
        "    @step\n",
        "    async def orchestrator(\n",
        "        self, ctx: Context, ev: OrchestratorEvent\n",
        "    ) -> ActiveSpeakerEvent | StopEvent:\n",
        "\n",
        "        agent_configs = await ctx.get(\"agent_configs\")\n",
        "        chat_history = await ctx.get(\"chat_history\")\n",
        "\n",
        "        agent_context_str = \"\"\n",
        "        for agent_name, agent_config in agent_configs.items():\n",
        "            agent_context_str += f\"{agent_name}: {agent_config.description}\\n\"\n",
        "\n",
        "        user_state = await ctx.get(\"user_state\")\n",
        "        user_state_str = \"\\n\".join([f\"{k}: {v}\" for k, v in user_state.items()])\n",
        "        system_prompt = self.orchestrator_prompt.format(\n",
        "            agent_context_str=agent_context_str, user_state_str=user_state_str\n",
        "        )\n",
        "\n",
        "        llm_input = [ChatMessage(role=\"system\", content=system_prompt)] + chat_history\n",
        "        llm = await ctx.get(\"llm\")\n",
        "\n",
        "\n",
        "        tools = [transfer_to_agent_tool]\n",
        "\n",
        "        response = await llm.achat_with_tools(tools, chat_history=llm_input)\n",
        "        tool_calls = llm.get_tool_calls_from_response(\n",
        "            response, error_on_no_tool_call=False\n",
        "        )\n",
        "\n",
        "\n",
        "        if len(tool_calls) == 0:\n",
        "            chat_history.append(response.message)\n",
        "            return StopEvent(\n",
        "                result={\n",
        "                    \"response\": response.message.content,\n",
        "                    \"chat_history\": chat_history,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        tool_call = tool_calls[0]\n",
        "        selected_agent = tool_call.tool_kwargs[\"agent_name\"]\n",
        "        await ctx.set(\"active_speaker\", selected_agent)\n",
        "\n",
        "        ctx.write_event_to_stream(\n",
        "            ProgressEvent(msg=f\"Transferring to agent {selected_agent}\")\n",
        "        )\n",
        "\n",
        "        return ActiveSpeakerEvent()"
      ],
      "metadata": {
        "id": "5m_K8hzG_mTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", temperature=0.3)\n",
        "workflow = ConciergeAgent(verbose=False)\n",
        "\n",
        "handler = workflow.run(\n",
        "    agent_configs=[agent_config, agent_config_2],\n",
        "    user_msg=\"What is 10 + 10?\",\n",
        "    chat_history=[],\n",
        "    initial_state={\"user_name\": \"Akshay\"},\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, ProgressEvent):\n",
        "        print(event.msg)\n",
        "    elif isinstance(event, ToolRequestEvent):\n",
        "        print(f\"Tool {event.tool_name} requires human approval. Approving!\")\n",
        "\n",
        "        handler.ctx.send_event(ToolApprovedEvent(\n",
        "            approved=True,\n",
        "            tool_name=event.tool_name,\n",
        "            tool_id=event.tool_id,\n",
        "            tool_kwargs=event.tool_kwargs,\n",
        "        ))\n",
        "\n",
        "final_result = await handler\n",
        "print(final_result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blefx88Xrhn9",
        "outputId": "faa206fe-0b16-41e9-e56e-ae119fb3f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferring to agent Addition Agent\n",
            "Tool add_two_numbers requires human approval. Approving!\n",
            "Tool add_two_numbers called with {'a': 10, 'b': 10} returned 20\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", temperature=0.3)\n",
        "workflow = ConciergeAgent(verbose=False)\n",
        "\n",
        "handler = workflow.run(\n",
        "    agent_configs=[agent_config, agent_config_2],\n",
        "    user_msg=\"What is 10 * 10?\",\n",
        "    chat_history=[],\n",
        "    initial_state={\"user_name\": \"Akshay\"},\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, ProgressEvent):\n",
        "        print(event.msg)\n",
        "    elif isinstance(event, ToolRequestEvent):\n",
        "        print(f\"Tool {event.tool_name} requires human approval. Approving!\")\n",
        "\n",
        "        handler.ctx.send_event(ToolApprovedEvent(\n",
        "            approved=True,\n",
        "            tool_name=event.tool_name,\n",
        "            tool_id=event.tool_id,\n",
        "            tool_kwargs=event.tool_kwargs,\n",
        "        ))\n",
        "\n",
        "final_result = await handler\n",
        "print(final_result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFqBojf_enX1",
        "outputId": "4ec5a601-d167-45d3-db38-48394c0db5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferring to agent Multiplication Agent\n",
            "Tool multiply_two_numbers called with {'a': 10, 'b': 10} returned 100\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", temperature=0.3)\n",
        "workflow = ConciergeAgent(verbose=False)\n",
        "\n",
        "handler = workflow.run(\n",
        "    agent_configs=[agent_config, agent_config_2],\n",
        "    user_msg=\"What is capital of india?\",\n",
        "    chat_history=[],\n",
        "    initial_state={\"user_name\": \"Akshay\"},\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, ProgressEvent):\n",
        "        print(event.msg)\n",
        "    elif isinstance(event, ToolRequestEvent):\n",
        "        print(f\"Tool {event.tool_name} requires human approval. Approving!\")\n",
        "\n",
        "        handler.ctx.send_event(ToolApprovedEvent(\n",
        "            approved=True,\n",
        "            tool_name=event.tool_name,\n",
        "            tool_id=event.tool_id,\n",
        "            tool_kwargs=event.tool_kwargs,\n",
        "        ))\n",
        "\n",
        "final_result = await handler\n",
        "print(final_result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8nmO7H5eq6D",
        "outputId": "060db2c5-0b01-4cfe-ca0b-0dc889bd381a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't answer this\n"
          ]
        }
      ]
    }
  ]
}